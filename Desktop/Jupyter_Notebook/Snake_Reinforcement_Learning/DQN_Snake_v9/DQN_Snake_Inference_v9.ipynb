{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1339f7e3-a9f6-44bf-890b-dcde7658fdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 - Score: 29, Total Reward: 1335.60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 405\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHighest Score in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m episodes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(scores)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 405\u001b[0m     \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 384\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(num_episodes, render)\u001b[0m\n\u001b[0;32m    381\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m--> 384\u001b[0m     \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# Capture the screen as an image\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     frame \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39marray3d(game\u001b[38;5;241m.\u001b[39mscreen)\n",
      "Cell \u001b[1;32mIn[11], line 250\u001b[0m, in \u001b[0;36mSnakeGame.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, RED, pygame\u001b[38;5;241m.\u001b[39mRect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfood[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfood[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_size))\n\u001b[0;32m    249\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import pygame\n",
    "import torch.nn.functional as F  # Import functional module\n",
    "import cv2\n",
    "\n",
    "# Define constants\n",
    "GRID_SIZE = 20\n",
    "PIXEL_SIZE = 30\n",
    "FOOD_REWARD = 50\n",
    "MOVE_PENALTY = -0.1\n",
    "CLOSER_REWARD = 1.2\n",
    "FURTHER_PENALTY = -1\n",
    "DEATH_PENALTY = -20\n",
    "TIME_PENALTY = -0.1\n",
    "MIN_EPSILON = 0.0\n",
    "FPS = 30\n",
    "EXPERIENCE_REPLAY_SIZE = 1000000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.8\n",
    "MODEL_SAVE_INTERVAL = 5000\n",
    "LEARNING_RATE = 0.00005\n",
    "EPISODES = 1000000  # Set the number of episodes for training\n",
    "STUCK_THRESHOLD = 2500  # Number of steps to consider the snake as stuck\n",
    "DECAY_RATE = 0.99999  # Epsilon decay rate\n",
    "STAGNATION_THRESHOLD = 50000  # Number of episodes for stagnation\n",
    "\n",
    "# Colors\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "\n",
    "class SnakeGame:\n",
    "    def __init__(self, render=True):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.pixel_size = PIXEL_SIZE\n",
    "        self.render_enabled = render\n",
    "        if self.render_enabled:\n",
    "            self.screen = pygame.display.set_mode((self.grid_size * self.pixel_size, self.grid_size * self.pixel_size))\n",
    "            pygame.display.set_caption('Improved Snake Game with DQN')\n",
    "        self.reset()\n",
    "        self.clock = pygame.time.Clock()  # For controlling the FPS\n",
    "\n",
    "    def reset(self):\n",
    "        self.snake = [(self.grid_size // 2, self.grid_size // 2)]\n",
    "        self.direction = (0, 1)  # Initial direction is moving right\n",
    "        self.place_food()\n",
    "        self.score = 0\n",
    "        self.steps_since_food = 0\n",
    "        self.previous_distance = self.get_distance(self.snake[0], self.food)\n",
    "        self.steps_since_last_progress = 0  # Initialize steps since last progress\n",
    "        return self.get_state()\n",
    "\n",
    "    def place_food(self):\n",
    "        while True:\n",
    "            self.food = (random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1))\n",
    "            if self.food not in self.snake:\n",
    "                break\n",
    "    \n",
    "    def get_state(self):\n",
    "        head_x, head_y = self.snake[0]\n",
    "        food_x, food_y = self.food\n",
    "\n",
    "        # Calculate normalized positions\n",
    "        head_x_norm = head_x / self.grid_size\n",
    "        head_y_norm = head_y / self.grid_size\n",
    "        food_x_norm = food_x / self.grid_size\n",
    "        food_y_norm = food_y / self.grid_size\n",
    "\n",
    "        # Direction vectors (normalized)\n",
    "        direction_x, direction_y = self.direction\n",
    "        magnitude_direction = np.sqrt(direction_x**2 + direction_y**2)\n",
    "        if magnitude_direction != 0:\n",
    "            direction_x /= magnitude_direction\n",
    "            direction_y /= magnitude_direction\n",
    "\n",
    "        # Distance to food (normalized)\n",
    "        distance_to_food = self.get_distance(self.snake[0], self.food) / (self.grid_size * 2)\n",
    "\n",
    "        # Angle to food (normalized)\n",
    "        relative_food_x = food_x - head_x\n",
    "        relative_food_y = food_y - head_y\n",
    "        dot_product = direction_x * relative_food_x + direction_y * relative_food_y\n",
    "        magnitude_food = np.sqrt(relative_food_x**2 + relative_food_y**2)\n",
    "        angle_to_food = np.arccos(np.clip(dot_product / magnitude_food, -1.0, 1.0)) / np.pi if magnitude_food != 0 else 0\n",
    "\n",
    "        # Normalize distances to the nearest walls\n",
    "        distance_to_left_wall = head_x / self.grid_size\n",
    "        distance_to_right_wall = (self.grid_size - 1 - head_x) / self.grid_size\n",
    "        distance_to_top_wall = head_y / self.grid_size\n",
    "        distance_to_bottom_wall = (self.grid_size - 1 - head_y) / self.grid_size\n",
    "\n",
    "        # Distance to the closest body segment\n",
    "        distances_to_body_segments = [self.get_distance(self.snake[0], segment) for segment in self.snake[1:]]\n",
    "        distance_to_closest_body_segment = min(distances_to_body_segments) / self.grid_size if distances_to_body_segments else 1\n",
    "\n",
    "        # Snake length (normalized)\n",
    "        snake_length = len(self.snake) / (self.grid_size * 2)\n",
    "\n",
    "        # Danger detection: Straight, Left, Right\n",
    "        danger_straight = self.is_collision((head_x + self.direction[0], head_y + self.direction[1]))\n",
    "\n",
    "        left_direction = (-self.direction[1], self.direction[0])  # Left turn\n",
    "        danger_left = self.is_collision((head_x + left_direction[0], head_y + left_direction[1]))\n",
    "\n",
    "        right_direction = (self.direction[1], -self.direction[0])  # Right turn\n",
    "        danger_right = self.is_collision((head_x + right_direction[0], head_y + right_direction[1]))\n",
    "\n",
    "        # Current direction (encoded as one-hot)\n",
    "        direction_left = int(self.direction == (-1, 0))\n",
    "        direction_right = int(self.direction == (1, 0))\n",
    "        direction_up = int(self.direction == (0, -1))\n",
    "        direction_down = int(self.direction == (0, 1))\n",
    "\n",
    "        # Calculate the relative direction of food\n",
    "        food_left = int(food_x < head_x)\n",
    "        food_right = int(food_x > head_x)\n",
    "        food_up = int(food_y < head_y)\n",
    "        food_down = int(food_y > head_y)\n",
    "\n",
    "        # Final state vector: Combined features\n",
    "        state = np.array([\n",
    "            head_x_norm, head_y_norm,                 # Snake head position (normalized)\n",
    "            food_x_norm, food_y_norm,                 # Food position (normalized)\n",
    "            direction_x, direction_y,                 # Snake direction\n",
    "            distance_to_food,                         # Distance to food\n",
    "            angle_to_food,                            # Angle to food\n",
    "            distance_to_left_wall, distance_to_right_wall, distance_to_top_wall, distance_to_bottom_wall,  # Wall distances (normalized)\n",
    "            distance_to_closest_body_segment,         # Closest body segment\n",
    "            snake_length,                             # Snake length (normalized)\n",
    "            danger_straight, danger_left, danger_right,  # Dangers\n",
    "            direction_left, direction_right, direction_up, direction_down,  # Current direction (one-hot encoded)\n",
    "            food_left, food_right, food_up, food_down  # Food relative direction\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.change_direction(action)\n",
    "        new_head = (self.snake[0][0] + self.direction[0], self.snake[0][1] + self.direction[1])\n",
    "    \n",
    "        reward = MOVE_PENALTY\n",
    "        done = False\n",
    "    \n",
    "        if self.is_collision(new_head):\n",
    "            reward = DEATH_PENALTY\n",
    "            done = True\n",
    "        else:\n",
    "            self.snake = [new_head] + self.snake[:-1]\n",
    "            new_distance = self.get_distance(self.snake[0], self.food)\n",
    "            self.previous_distance = new_distance\n",
    "            self.steps_since_food += 1\n",
    "    \n",
    "            if new_head == self.food:\n",
    "                self.snake = [new_head] + self.snake\n",
    "                self.score += 1  # Update the score\n",
    "                reward = FOOD_REWARD\n",
    "                self.place_food()\n",
    "                self.steps_since_food = 0\n",
    "                self.previous_distance = self.get_distance(self.snake[0], self.food)\n",
    "            else:\n",
    "                old_distance = self.previous_distance\n",
    "                if new_distance < old_distance:\n",
    "                    reward += CLOSER_REWARD\n",
    "                else:\n",
    "                    reward += FURTHER_PENALTY\n",
    "    \n",
    "                reward += - (self.steps_since_food * TIME_PENALTY)\n",
    "    \n",
    "                if self.steps_since_last_progress >= STUCK_THRESHOLD:\n",
    "                    if self.detect_loop() or self.is_in_small_area():\n",
    "                        done = True\n",
    "                else:\n",
    "                    if new_distance != old_distance:\n",
    "                        self.steps_since_last_progress = 0\n",
    "                    else:\n",
    "                        self.steps_since_last_progress += 1\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "    \n",
    "    def detect_loop(self):\n",
    "        recent_positions = deque(maxlen=20)  # Track last 20 positions\n",
    "        recent_positions.append(self.snake[0])\n",
    "        \n",
    "        # Check if snake is revisiting recent positions too frequently\n",
    "        revisit_count = sum(1 for pos in self.snake[1:] if pos in recent_positions)\n",
    "        if revisit_count >= len(self.snake) / 2:  # Arbitrary threshold; adjust as needed\n",
    "            return True\n",
    "        \n",
    "        # Update previous distance\n",
    "        current_distance = self.get_distance(self.snake[0], self.food)\n",
    "        if abs(current_distance - self.previous_distance) < 1:\n",
    "            return True\n",
    "    \n",
    "        # Update previous distance for next check\n",
    "        self.previous_distance = current_distance\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def is_in_small_area(self):\n",
    "        # Check if the snake is confined to a small area\n",
    "        snake_positions = set(self.snake)\n",
    "        min_x = min(x for x, y in snake_positions)\n",
    "        max_x = max(x for x, y in snake_positions)\n",
    "        min_y = min(y for x, y in snake_positions)\n",
    "        max_y = max(y for x, y in snake_positions)\n",
    "        return (max_x - min_x <= 10) and (max_y - min_y <= 10)  # Adjust the area size threshold\n",
    "\n",
    "    def change_direction(self, action):\n",
    "        left_turns = {\n",
    "            (1, 0): (0, 1),   # Moving right, turn left -> down\n",
    "            (0, 1): (-1, 0),  # Moving down, turn left -> left\n",
    "            (-1, 0): (0, -1), # Moving left, turn left -> up\n",
    "            (0, -1): (1, 0)   # Moving up, turn left -> right\n",
    "        }\n",
    "        \n",
    "        right_turns = {\n",
    "            (1, 0): (0, -1),  # Moving right, turn right -> up\n",
    "            (0, 1): (1, 0),   # Moving down, turn right -> right\n",
    "            (-1, 0): (0, 1),  # Moving left, turn right -> down\n",
    "            (0, -1): (-1, 0)  # Moving up, turn right -> left\n",
    "        }\n",
    "        \n",
    "        if action == 0:  # Left\n",
    "            self.direction = left_turns[self.direction]\n",
    "        elif action == 1:  # Right\n",
    "            self.direction = right_turns[self.direction]\n",
    "        # No action means move straight (no change in direction)\n",
    "\n",
    "    def is_collision(self, position):\n",
    "        x, y = position\n",
    "        return (x < 0 or x >= self.grid_size or y < 0 or y >= self.grid_size or\n",
    "                position in self.snake)\n",
    "    \n",
    "    def get_distance(self, pos1, pos2):\n",
    "        x1, y1 = pos1\n",
    "        x2, y2 = pos2\n",
    "        return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "    \n",
    "    def render(self):\n",
    "        self.screen.fill(BLACK)\n",
    "        for segment in self.snake:\n",
    "            pygame.draw.rect(self.screen, GREEN, pygame.Rect(segment[0] * self.pixel_size, segment[1] * self.pixel_size, self.pixel_size, self.pixel_size))\n",
    "        pygame.draw.rect(self.screen, RED, pygame.Rect(self.food[0] * self.pixel_size, self.food[1] * self.pixel_size, self.pixel_size, self.pixel_size))\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(FPS)  # Cap the frame rate\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)  # Initial number of units\n",
    "        self.ln1 = nn.LayerNorm(1024)  # Use LayerNorm\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Dropout rate\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 1024)  # Fixed input size to match output of fc1\n",
    "        self.ln2 = nn.LayerNorm(1024)  # Use LayerNorm\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Dropout rate\n",
    "\n",
    "        self.fc3 = nn.Linear(1024, 1024)  # Fixed input size to match output of fc2\n",
    "        self.ln3 = nn.LayerNorm(1024)  # Use LayerNorm\n",
    "        self.dropout3 = nn.Dropout(0.1)  # Dropout rate\n",
    "\n",
    "        self.fc4 = nn.Linear(1024, output_dim)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.ln1(self.fc1(x)), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.ln2(self.fc2(x)), negative_slope=0.01)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.ln3(self.fc3(x)), negative_slope=0.01)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, action_dim, device):\n",
    "        self.device = device\n",
    "        self.action_dim = action_dim\n",
    "        self.q_network = DQN(input_dim, action_dim).to(device)\n",
    "        self.target_network = DQN(input_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=LEARNING_RATE)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10000, gamma=0.9)\n",
    "        self.replay_buffer = deque(maxlen=EXPERIENCE_REPLAY_SIZE)\n",
    "        self.update_target_network()\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)  # Random action\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return q_values.argmax().item()  # Best action\n",
    "\n",
    "    def store_experience(self, experience):\n",
    "        self.replay_buffer.append(experience)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return None  # Not enough data to train\n",
    "    \n",
    "        batch = random.sample(self.replay_buffer, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32).to(self.device)\n",
    "    \n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "        # Double DQN\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.q_network(next_states).argmax(1)\n",
    "            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "    \n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)  # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()  # Step the learning rate scheduler\n",
    "    \n",
    "        return loss.item()  # Return the loss value\n",
    "\n",
    "def decay_epsilon(epsilon, episode, decay_rate=DECAY_RATE, min_epsilon=MIN_EPSILON):\n",
    "    return max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "def run_inference(num_episodes=10, render=True):\n",
    "    pygame.init()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_dim = 25\n",
    "    action_dim = 3\n",
    "    agent = DQNAgent(input_dim, action_dim, device)\n",
    "    game = SnakeGame(render=render)\n",
    "\n",
    "    # Load the trained model\n",
    "    model_path = \"snake_dqn_model_470000.pth\"\n",
    "    try:\n",
    "        agent.q_network.load_state_dict(torch.load(model_path))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model file {model_path} not found.\")\n",
    "        pygame.quit()\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        pygame.quit()\n",
    "        return\n",
    "\n",
    "    agent.q_network.eval()\n",
    "\n",
    "    scores = []\n",
    "    epsilon = 0.0  # Use epsilon = 0 during inference (purely greedy)\n",
    "\n",
    "    # Initialize video writer if rendering\n",
    "    if render:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for AVI files; use 'MP4V' for MP4 files\n",
    "        video_writer = cv2.VideoWriter('snake_game_playback.avi', fourcc, FPS, (game.grid_size * game.pixel_size, game.grid_size * game.pixel_size))\n",
    "\n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state = game.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                q_values = agent.q_network(state_tensor)\n",
    "                action = agent.select_action(state, epsilon)  # Pass epsilon value\n",
    "\n",
    "                next_state, reward, done = game.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if render:\n",
    "                    game.render()\n",
    "                    # Capture the screen as an image\n",
    "                    frame = pygame.surfarray.array3d(game.screen)\n",
    "                    frame = np.transpose(frame, (1, 0, 2))  # Convert from (width, height, channels) to (height, width, channels)\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert from RGB to BGR\n",
    "                    video_writer.write(frame)  # Write the frame to the video file\n",
    "\n",
    "                game.clock.tick(FPS)  # Control the frame rate\n",
    "\n",
    "            scores.append(game.score)\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} - Score: {game.score}, Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "    finally:\n",
    "        pygame.quit()\n",
    "        if render:\n",
    "            video_writer.release()  # Ensure the video writer is released\n",
    "\n",
    "    print(f\"Average Score over {num_episodes} episodes: {sum(scores) / num_episodes:.2f}\")\n",
    "    print(f\"Highest Score in {num_episodes} episodes: {max(scores)}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run_inference(num_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08805ef0-b149-4bdf-bd1b-7995c30d87cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee9f4f-8c08-492f-89de-03b89a5a9e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498d343-6d19-44f9-a2ee-e355da5521a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
