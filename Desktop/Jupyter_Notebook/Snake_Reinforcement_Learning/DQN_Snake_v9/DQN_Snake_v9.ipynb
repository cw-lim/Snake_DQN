{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2143446-2856-4e66-8da1-df6af2f4bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Loss: Not computed due to insufficient experience\n",
      "New best score: 1. Model saved to best_snake_dqn_model.pth\n",
      "New best score: 2. Model saved to best_snake_dqn_model.pth\n",
      "Episode 1000, Loss: 13.8122, Total Reward: 865.60, Epsilon: 0.99\n",
      "Episode 2000, Loss: 11.7830, Total Reward: 421.00, Epsilon: 0.98\n",
      "Episode 3000, Loss: 17.9022, Total Reward: 78.00, Epsilon: 0.97\n",
      "Episode 4000, Loss: 20.6368, Total Reward: 324.80, Epsilon: 0.96\n",
      "Model saved to snake_dqn_model_5000.pth\n",
      "Episode 5000, Loss: 15.1273, Total Reward: 97.00, Epsilon: 0.95\n",
      "New best score: 3. Model saved to best_snake_dqn_model.pth\n",
      "Episode 6000, Loss: 21.2774, Total Reward: 102.00, Epsilon: 0.94\n",
      "Episode 7000, Loss: 29.9042, Total Reward: -8.40, Epsilon: 0.93\n",
      "Episode 8000, Loss: 23.5034, Total Reward: -24.00, Epsilon: 0.92\n",
      "Episode 9000, Loss: 22.9972, Total Reward: 7.00, Epsilon: 0.91\n",
      "Model saved to snake_dqn_model_10000.pth\n",
      "Episode 10000, Loss: 29.1298, Total Reward: 82.60, Epsilon: 0.90\n",
      "Episode 11000, Loss: 29.4877, Total Reward: 151.50, Epsilon: 0.90\n",
      "Episode 12000, Loss: 17.7143, Total Reward: -16.40, Epsilon: 0.89\n",
      "Episode 13000, Loss: 30.9998, Total Reward: 92.10, Epsilon: 0.88\n",
      "Episode 14000, Loss: 22.3792, Total Reward: 64.80, Epsilon: 0.87\n",
      "Model saved to snake_dqn_model_15000.pth\n",
      "Episode 15000, Loss: 18.8472, Total Reward: 459.60, Epsilon: 0.86\n",
      "Episode 16000, Loss: 15.5658, Total Reward: 195.60, Epsilon: 0.85\n",
      "Episode 17000, Loss: 25.8322, Total Reward: 87.30, Epsilon: 0.84\n",
      "Episode 18000, Loss: 13.7190, Total Reward: 42.10, Epsilon: 0.84\n",
      "Episode 19000, Loss: 18.4905, Total Reward: 585.00, Epsilon: 0.83\n",
      "Model saved to snake_dqn_model_20000.pth\n",
      "Episode 20000, Loss: 19.3289, Total Reward: 110.60, Epsilon: 0.82\n",
      "Episode 21000, Loss: 18.6004, Total Reward: 58.90, Epsilon: 0.81\n",
      "Episode 22000, Loss: 15.1147, Total Reward: -24.90, Epsilon: 0.80\n",
      "New best score: 4. Model saved to best_snake_dqn_model.pth\n",
      "Episode 23000, Loss: 16.5483, Total Reward: 25.60, Epsilon: 0.79\n",
      "Episode 24000, Loss: 13.1518, Total Reward: 41.70, Epsilon: 0.79\n",
      "Model saved to snake_dqn_model_25000.pth\n",
      "Episode 25000, Loss: 19.0196, Total Reward: 67.60, Epsilon: 0.78\n",
      "Episode 26000, Loss: 15.2770, Total Reward: 41.10, Epsilon: 0.77\n",
      "New best score: 5. Model saved to best_snake_dqn_model.pth\n",
      "Episode 27000, Loss: 15.1101, Total Reward: 117.60, Epsilon: 0.76\n",
      "Episode 28000, Loss: 16.4174, Total Reward: 418.50, Epsilon: 0.76\n",
      "Episode 29000, Loss: 15.2347, Total Reward: -11.90, Epsilon: 0.75\n",
      "Model saved to snake_dqn_model_30000.pth\n",
      "Episode 30000, Loss: 16.9587, Total Reward: 468.30, Epsilon: 0.74\n",
      "Episode 31000, Loss: 13.3890, Total Reward: 21.00, Epsilon: 0.73\n",
      "Episode 32000, Loss: 17.3446, Total Reward: 175.40, Epsilon: 0.73\n",
      "Episode 33000, Loss: 16.9778, Total Reward: 34.00, Epsilon: 0.72\n",
      "Episode 34000, Loss: 20.1588, Total Reward: 7.00, Epsilon: 0.71\n",
      "Model saved to snake_dqn_model_35000.pth\n",
      "Episode 35000, Loss: 15.5303, Total Reward: 76.40, Epsilon: 0.70\n",
      "Episode 36000, Loss: 15.1864, Total Reward: 36.40, Epsilon: 0.70\n",
      "Episode 37000, Loss: 21.4958, Total Reward: 20.50, Epsilon: 0.69\n",
      "Episode 38000, Loss: 17.8008, Total Reward: -2.40, Epsilon: 0.68\n",
      "Episode 39000, Loss: 16.4160, Total Reward: 56.50, Epsilon: 0.68\n",
      "Model saved to snake_dqn_model_40000.pth\n",
      "Episode 40000, Loss: 24.7598, Total Reward: -13.50, Epsilon: 0.67\n",
      "Episode 41000, Loss: 23.4898, Total Reward: 170.00, Epsilon: 0.66\n",
      "Episode 42000, Loss: 13.3491, Total Reward: 116.50, Epsilon: 0.66\n",
      "Episode 43000, Loss: 18.1335, Total Reward: -4.50, Epsilon: 0.65\n",
      "New best score: 6. Model saved to best_snake_dqn_model.pth\n",
      "Episode 44000, Loss: 12.6808, Total Reward: 271.20, Epsilon: 0.64\n",
      "Model saved to snake_dqn_model_45000.pth\n",
      "Episode 45000, Loss: 8.4226, Total Reward: 87.60, Epsilon: 0.64\n",
      "Episode 46000, Loss: 13.9374, Total Reward: -24.90, Epsilon: 0.63\n",
      "Episode 47000, Loss: 20.0313, Total Reward: 30.40, Epsilon: 0.63\n",
      "Episode 48000, Loss: 15.7549, Total Reward: 24.50, Epsilon: 0.62\n",
      "Episode 49000, Loss: 13.9097, Total Reward: 212.60, Epsilon: 0.61\n",
      "Model saved to snake_dqn_model_50000.pth\n",
      "Episode 50000, Loss: 12.9835, Total Reward: 184.60, Epsilon: 0.61\n",
      "Episode 51000, Loss: 9.3135, Total Reward: -18.90, Epsilon: 0.60\n",
      "Episode 52000, Loss: 13.8564, Total Reward: 42.10, Epsilon: 0.59\n",
      "New best score: 7. Model saved to best_snake_dqn_model.pth\n",
      "Episode 53000, Loss: 16.6109, Total Reward: 4.50, Epsilon: 0.59\n",
      "Episode 54000, Loss: 10.5860, Total Reward: 133.00, Epsilon: 0.58\n",
      "Model saved to snake_dqn_model_55000.pth\n",
      "Episode 55000, Loss: 11.7949, Total Reward: 22.10, Epsilon: 0.58\n",
      "Episode 56000, Loss: 12.6978, Total Reward: 68.60, Epsilon: 0.57\n",
      "Episode 57000, Loss: 19.3412, Total Reward: 221.80, Epsilon: 0.57\n",
      "Episode 58000, Loss: 14.8797, Total Reward: 22.70, Epsilon: 0.56\n",
      "New best score: 8. Model saved to best_snake_dqn_model.pth\n",
      "Episode 59000, Loss: 10.3044, Total Reward: 215.30, Epsilon: 0.55\n",
      "Model saved to snake_dqn_model_60000.pth\n",
      "Episode 60000, Loss: 9.2583, Total Reward: -16.40, Epsilon: 0.55\n",
      "New best score: 9. Model saved to best_snake_dqn_model.pth\n",
      "Episode 61000, Loss: 17.0383, Total Reward: 197.10, Epsilon: 0.54\n",
      "Episode 62000, Loss: 15.8378, Total Reward: 60.40, Epsilon: 0.54\n",
      "Episode 63000, Loss: 12.2094, Total Reward: 146.10, Epsilon: 0.53\n",
      "Episode 64000, Loss: 19.5882, Total Reward: 67.90, Epsilon: 0.53\n",
      "Model saved to snake_dqn_model_65000.pth\n",
      "Episode 65000, Loss: 12.2313, Total Reward: -24.50, Epsilon: 0.52\n",
      "Episode 66000, Loss: 11.3715, Total Reward: -6.50, Epsilon: 0.52\n",
      "Episode 67000, Loss: 11.9274, Total Reward: 222.00, Epsilon: 0.51\n",
      "Episode 68000, Loss: 12.9504, Total Reward: 182.50, Epsilon: 0.51\n",
      "Episode 69000, Loss: 10.9074, Total Reward: 18.00, Epsilon: 0.50\n",
      "Model saved to snake_dqn_model_70000.pth\n",
      "Episode 70000, Loss: 11.5647, Total Reward: 67.00, Epsilon: 0.50\n",
      "Episode 71000, Loss: 13.7676, Total Reward: 192.10, Epsilon: 0.49\n",
      "Episode 72000, Loss: 13.5762, Total Reward: 183.10, Epsilon: 0.49\n",
      "Episode 73000, Loss: 19.0784, Total Reward: 166.50, Epsilon: 0.48\n",
      "Episode 74000, Loss: 10.3574, Total Reward: 376.90, Epsilon: 0.48\n",
      "Model saved to snake_dqn_model_75000.pth\n",
      "Episode 75000, Loss: 11.0910, Total Reward: 293.20, Epsilon: 0.47\n",
      "Episode 76000, Loss: 12.5672, Total Reward: 112.50, Epsilon: 0.47\n",
      "Episode 77000, Loss: 12.1642, Total Reward: 23.80, Epsilon: 0.46\n",
      "Episode 78000, Loss: 15.7247, Total Reward: 290.50, Epsilon: 0.46\n",
      "Episode 79000, Loss: 13.8815, Total Reward: 504.80, Epsilon: 0.45\n",
      "Model saved to snake_dqn_model_80000.pth\n",
      "Episode 80000, Loss: 11.1339, Total Reward: 1549.80, Epsilon: 0.45\n",
      "Episode 81000, Loss: 19.6916, Total Reward: -21.00, Epsilon: 0.44\n",
      "Episode 82000, Loss: 7.9360, Total Reward: 136.80, Epsilon: 0.44\n",
      "Episode 83000, Loss: 13.6162, Total Reward: 23.90, Epsilon: 0.44\n",
      "Episode 84000, Loss: 12.2448, Total Reward: -0.20, Epsilon: 0.43\n",
      "New best score: 10. Model saved to best_snake_dqn_model.pth\n",
      "Model saved to snake_dqn_model_85000.pth\n",
      "Episode 85000, Loss: 12.2851, Total Reward: 87.70, Epsilon: 0.43\n",
      "Episode 86000, Loss: 12.6550, Total Reward: 18.00, Epsilon: 0.42\n",
      "Episode 87000, Loss: 8.2769, Total Reward: 210.70, Epsilon: 0.42\n",
      "Episode 88000, Loss: 13.5868, Total Reward: 157.70, Epsilon: 0.41\n",
      "Episode 89000, Loss: 9.6033, Total Reward: 273.20, Epsilon: 0.41\n",
      "Model saved to snake_dqn_model_90000.pth\n",
      "Episode 90000, Loss: 10.3706, Total Reward: 283.10, Epsilon: 0.41\n",
      "New best score: 11. Model saved to best_snake_dqn_model.pth\n",
      "Episode 91000, Loss: 9.2753, Total Reward: 169.80, Epsilon: 0.40\n",
      "Episode 92000, Loss: 10.8882, Total Reward: -4.50, Epsilon: 0.40\n",
      "Episode 93000, Loss: 9.7786, Total Reward: 76.10, Epsilon: 0.39\n",
      "New best score: 13. Model saved to best_snake_dqn_model.pth\n",
      "New best score: 14. Model saved to best_snake_dqn_model.pth\n",
      "Episode 94000, Loss: 12.4757, Total Reward: 27.10, Epsilon: 0.39\n",
      "Model saved to snake_dqn_model_95000.pth\n",
      "Episode 95000, Loss: 9.0027, Total Reward: -24.90, Epsilon: 0.39\n",
      "Episode 96000, Loss: 9.2610, Total Reward: 51.60, Epsilon: 0.38\n",
      "Episode 97000, Loss: 13.1941, Total Reward: -24.00, Epsilon: 0.38\n",
      "Episode 98000, Loss: 8.1340, Total Reward: 139.20, Epsilon: 0.38\n",
      "Episode 99000, Loss: 14.0588, Total Reward: 348.80, Epsilon: 0.37\n",
      "Model saved to snake_dqn_model_100000.pth\n",
      "Episode 100000, Loss: 14.9271, Total Reward: 173.00, Epsilon: 0.37\n",
      "Episode 101000, Loss: 11.0961, Total Reward: 18.00, Epsilon: 0.36\n",
      "Episode 102000, Loss: 15.2359, Total Reward: 216.70, Epsilon: 0.36\n",
      "Episode 103000, Loss: 12.9305, Total Reward: 184.10, Epsilon: 0.36\n",
      "Episode 104000, Loss: 7.4600, Total Reward: 56.50, Epsilon: 0.35\n",
      "Model saved to snake_dqn_model_105000.pth\n",
      "Episode 105000, Loss: 19.7176, Total Reward: 272.60, Epsilon: 0.35\n",
      "Episode 106000, Loss: 13.9672, Total Reward: 120.10, Epsilon: 0.35\n",
      "Episode 107000, Loss: 8.0799, Total Reward: 30.10, Epsilon: 0.34\n",
      "Episode 108000, Loss: 16.8935, Total Reward: 221.30, Epsilon: 0.34\n",
      "Episode 109000, Loss: 11.9147, Total Reward: 250.60, Epsilon: 0.34\n",
      "Model saved to snake_dqn_model_110000.pth\n",
      "Episode 110000, Loss: 12.2395, Total Reward: 542.50, Epsilon: 0.33\n",
      "Episode 111000, Loss: 13.0506, Total Reward: 69.40, Epsilon: 0.33\n",
      "Episode 112000, Loss: 12.2191, Total Reward: 1695.00, Epsilon: 0.33\n",
      "Episode 113000, Loss: 9.8459, Total Reward: 68.50, Epsilon: 0.32\n",
      "Episode 114000, Loss: 10.6730, Total Reward: 76.10, Epsilon: 0.32\n",
      "Model saved to snake_dqn_model_115000.pth\n",
      "Episode 115000, Loss: 10.2665, Total Reward: -11.90, Epsilon: 0.32\n",
      "Episode 116000, Loss: 8.5846, Total Reward: 20.60, Epsilon: 0.31\n",
      "Episode 117000, Loss: 14.1597, Total Reward: 127.60, Epsilon: 0.31\n",
      "Episode 118000, Loss: 14.9462, Total Reward: 194.00, Epsilon: 0.31\n",
      "Episode 119000, Loss: 8.6386, Total Reward: 112.70, Epsilon: 0.30\n",
      "Model saved to snake_dqn_model_120000.pth\n",
      "Episode 120000, Loss: 10.9736, Total Reward: 71.70, Epsilon: 0.30\n",
      "Episode 121000, Loss: 9.2413, Total Reward: 70.80, Epsilon: 0.30\n",
      "Episode 122000, Loss: 10.5251, Total Reward: 34.00, Epsilon: 0.30\n",
      "Episode 123000, Loss: 11.6813, Total Reward: 65.00, Epsilon: 0.29\n",
      "New best score: 15. Model saved to best_snake_dqn_model.pth\n",
      "Episode 124000, Loss: 11.0613, Total Reward: 252.90, Epsilon: 0.29\n",
      "Model saved to snake_dqn_model_125000.pth\n",
      "Episode 125000, Loss: 18.4146, Total Reward: 260.60, Epsilon: 0.29\n",
      "Episode 126000, Loss: 18.3395, Total Reward: 301.50, Epsilon: 0.28\n",
      "Episode 127000, Loss: 13.2231, Total Reward: 172.60, Epsilon: 0.28\n",
      "Episode 128000, Loss: 9.9468, Total Reward: 132.20, Epsilon: 0.28\n",
      "Episode 129000, Loss: 14.8528, Total Reward: 23.60, Epsilon: 0.28\n",
      "New best score: 17. Model saved to best_snake_dqn_model.pth\n",
      "Model saved to snake_dqn_model_130000.pth\n",
      "Episode 130000, Loss: 7.0128, Total Reward: 240.20, Epsilon: 0.27\n",
      "Episode 131000, Loss: 10.1547, Total Reward: 121.20, Epsilon: 0.27\n",
      "Episode 132000, Loss: 12.4631, Total Reward: 275.40, Epsilon: 0.27\n",
      "Episode 133000, Loss: 9.2080, Total Reward: 134.10, Epsilon: 0.26\n",
      "Episode 134000, Loss: 16.4211, Total Reward: 428.00, Epsilon: 0.26\n",
      "Model saved to snake_dqn_model_135000.pth\n",
      "Episode 135000, Loss: 11.7317, Total Reward: 186.80, Epsilon: 0.26\n",
      "Episode 136000, Loss: 9.6617, Total Reward: 49.30, Epsilon: 0.26\n",
      "Episode 137000, Loss: 17.7525, Total Reward: 538.70, Epsilon: 0.25\n",
      "Episode 138000, Loss: 6.9319, Total Reward: 111.10, Epsilon: 0.25\n",
      "Episode 139000, Loss: 19.3462, Total Reward: 159.10, Epsilon: 0.25\n",
      "New best score: 20. Model saved to best_snake_dqn_model.pth\n",
      "Model saved to snake_dqn_model_140000.pth\n",
      "Episode 140000, Loss: 22.2489, Total Reward: 462.20, Epsilon: 0.25\n",
      "Episode 141000, Loss: 10.4236, Total Reward: 25.40, Epsilon: 0.24\n",
      "Episode 142000, Loss: 14.8077, Total Reward: 248.30, Epsilon: 0.24\n",
      "Episode 143000, Loss: 15.3661, Total Reward: 300.70, Epsilon: 0.24\n",
      "Episode 144000, Loss: 8.5341, Total Reward: 155.40, Epsilon: 0.24\n",
      "Model saved to snake_dqn_model_145000.pth\n",
      "Episode 145000, Loss: 11.8284, Total Reward: 48.60, Epsilon: 0.23\n",
      "Episode 146000, Loss: 8.7150, Total Reward: 414.80, Epsilon: 0.23\n",
      "Episode 147000, Loss: 11.9450, Total Reward: 302.60, Epsilon: 0.23\n",
      "Episode 148000, Loss: 8.2813, Total Reward: 132.60, Epsilon: 0.23\n",
      "Episode 149000, Loss: 12.4856, Total Reward: 52.50, Epsilon: 0.23\n",
      "Model saved to snake_dqn_model_150000.pth\n",
      "Episode 150000, Loss: 6.9153, Total Reward: 277.30, Epsilon: 0.22\n",
      "Episode 151000, Loss: 10.9479, Total Reward: 170.70, Epsilon: 0.22\n",
      "Episode 152000, Loss: 12.1200, Total Reward: 189.00, Epsilon: 0.22\n",
      "Episode 153000, Loss: 19.8594, Total Reward: 19.60, Epsilon: 0.22\n",
      "New best score: 21. Model saved to best_snake_dqn_model.pth\n",
      "New best score: 22. Model saved to best_snake_dqn_model.pth\n",
      "Episode 154000, Loss: 9.0606, Total Reward: -11.90, Epsilon: 0.21\n",
      "Model saved to snake_dqn_model_155000.pth\n",
      "Episode 155000, Loss: 10.6499, Total Reward: 321.30, Epsilon: 0.21\n",
      "Episode 156000, Loss: 9.1442, Total Reward: 1277.90, Epsilon: 0.21\n",
      "Episode 157000, Loss: 11.5287, Total Reward: 230.10, Epsilon: 0.21\n",
      "Episode 158000, Loss: 12.9027, Total Reward: 172.20, Epsilon: 0.21\n",
      "Episode 159000, Loss: 12.1903, Total Reward: 52.10, Epsilon: 0.20\n",
      "Model saved to snake_dqn_model_160000.pth\n",
      "Episode 160000, Loss: 17.5979, Total Reward: 112.10, Epsilon: 0.20\n",
      "Episode 161000, Loss: 8.3969, Total Reward: 52.50, Epsilon: 0.20\n",
      "Episode 162000, Loss: 11.4121, Total Reward: 591.90, Epsilon: 0.20\n",
      "Episode 163000, Loss: 7.4132, Total Reward: -18.90, Epsilon: 0.20\n",
      "Episode 164000, Loss: 8.0659, Total Reward: 65.20, Epsilon: 0.19\n",
      "Model saved to snake_dqn_model_165000.pth\n",
      "Episode 165000, Loss: 9.2456, Total Reward: 23.60, Epsilon: 0.19\n",
      "Episode 166000, Loss: 11.2051, Total Reward: 254.00, Epsilon: 0.19\n",
      "Episode 167000, Loss: 7.0962, Total Reward: 3289.80, Epsilon: 0.19\n",
      "Episode 168000, Loss: 13.5599, Total Reward: 504.10, Epsilon: 0.19\n",
      "Episode 169000, Loss: 7.7068, Total Reward: 182.70, Epsilon: 0.18\n",
      "Model saved to snake_dqn_model_170000.pth\n",
      "Episode 170000, Loss: 13.4100, Total Reward: 210.20, Epsilon: 0.18\n",
      "Episode 171000, Loss: 9.0365, Total Reward: 319.50, Epsilon: 0.18\n",
      "Episode 172000, Loss: 7.1915, Total Reward: 21.80, Epsilon: 0.18\n",
      "Episode 173000, Loss: 8.8657, Total Reward: 67.60, Epsilon: 0.18\n",
      "Episode 174000, Loss: 18.5412, Total Reward: 987.60, Epsilon: 0.18\n",
      "New best score: 24. Model saved to best_snake_dqn_model.pth\n",
      "Model saved to snake_dqn_model_175000.pth\n",
      "Episode 175000, Loss: 11.4905, Total Reward: 400.50, Epsilon: 0.17\n",
      "Episode 176000, Loss: 17.8260, Total Reward: 182.20, Epsilon: 0.17\n",
      "Episode 177000, Loss: 23.8552, Total Reward: 202.30, Epsilon: 0.17\n",
      "Episode 178000, Loss: 17.3489, Total Reward: -13.50, Epsilon: 0.17\n",
      "Episode 179000, Loss: 11.2561, Total Reward: 23.20, Epsilon: 0.17\n",
      "Model saved to snake_dqn_model_180000.pth\n",
      "Episode 180000, Loss: 16.0915, Total Reward: 814.50, Epsilon: 0.17\n",
      "Episode 181000, Loss: 11.7590, Total Reward: 111.20, Epsilon: 0.16\n",
      "Episode 182000, Loss: 15.7445, Total Reward: 697.40, Epsilon: 0.16\n",
      "Episode 183000, Loss: 14.4398, Total Reward: 166.00, Epsilon: 0.16\n",
      "Episode 184000, Loss: 14.7552, Total Reward: 19.70, Epsilon: 0.16\n",
      "Model saved to snake_dqn_model_185000.pth\n",
      "Episode 185000, Loss: 12.9192, Total Reward: 554.50, Epsilon: 0.16\n",
      "Episode 186000, Loss: 11.4032, Total Reward: 83.80, Epsilon: 0.16\n",
      "Episode 187000, Loss: 26.5730, Total Reward: 113.50, Epsilon: 0.15\n",
      "Episode 188000, Loss: 8.0082, Total Reward: 501.40, Epsilon: 0.15\n",
      "Episode 189000, Loss: 8.8068, Total Reward: 266.90, Epsilon: 0.15\n",
      "Model saved to snake_dqn_model_190000.pth\n",
      "Episode 190000, Loss: 14.3776, Total Reward: 36.40, Epsilon: 0.15\n",
      "Episode 191000, Loss: 10.4996, Total Reward: 256.90, Epsilon: 0.15\n",
      "Episode 192000, Loss: 17.9869, Total Reward: 483.00, Epsilon: 0.15\n",
      "Episode 193000, Loss: 12.3359, Total Reward: 122.60, Epsilon: 0.15\n",
      "Episode 194000, Loss: 19.7965, Total Reward: 321.60, Epsilon: 0.14\n",
      "Model saved to snake_dqn_model_195000.pth\n",
      "Episode 195000, Loss: 18.2873, Total Reward: 376.70, Epsilon: 0.14\n",
      "Episode 196000, Loss: 10.4304, Total Reward: 287.80, Epsilon: 0.14\n",
      "Episode 197000, Loss: 18.4049, Total Reward: -6.50, Epsilon: 0.14\n",
      "Episode 198000, Loss: 17.2392, Total Reward: 588.00, Epsilon: 0.14\n",
      "Episode 199000, Loss: 17.8904, Total Reward: 21.90, Epsilon: 0.14\n",
      "New best score: 26. Model saved to best_snake_dqn_model.pth\n",
      "Model saved to snake_dqn_model_200000.pth\n",
      "Episode 200000, Loss: 15.9454, Total Reward: 401.90, Epsilon: 0.14\n",
      "New best score: 27. Model saved to best_snake_dqn_model.pth\n",
      "Episode 201000, Loss: 14.1310, Total Reward: 68.30, Epsilon: 0.13\n",
      "Episode 202000, Loss: 9.2677, Total Reward: 526.30, Epsilon: 0.13\n",
      "Episode 203000, Loss: 13.9432, Total Reward: 310.00, Epsilon: 0.13\n",
      "Episode 204000, Loss: 26.3002, Total Reward: 390.70, Epsilon: 0.13\n",
      "Model saved to snake_dqn_model_205000.pth\n",
      "Episode 205000, Loss: 13.1422, Total Reward: 164.00, Epsilon: 0.13\n",
      "Episode 206000, Loss: 13.6478, Total Reward: 254.30, Epsilon: 0.13\n",
      "Episode 207000, Loss: 10.2243, Total Reward: 707.10, Epsilon: 0.13\n",
      "Episode 208000, Loss: 9.5480, Total Reward: 114.00, Epsilon: 0.12\n",
      "Episode 209000, Loss: 11.7868, Total Reward: 422.60, Epsilon: 0.12\n",
      "Model saved to snake_dqn_model_210000.pth\n",
      "Episode 210000, Loss: 9.5769, Total Reward: 222.20, Epsilon: 0.12\n",
      "Episode 211000, Loss: 8.5681, Total Reward: 546.70, Epsilon: 0.12\n",
      "Episode 212000, Loss: 10.1928, Total Reward: 228.30, Epsilon: 0.12\n",
      "Episode 213000, Loss: 14.2950, Total Reward: 67.40, Epsilon: 0.12\n",
      "Episode 214000, Loss: 11.8313, Total Reward: 118.70, Epsilon: 0.12\n",
      "Model saved to snake_dqn_model_215000.pth\n",
      "Episode 215000, Loss: 16.9332, Total Reward: 21.50, Epsilon: 0.12\n",
      "Episode 216000, Loss: 14.6984, Total Reward: 110.50, Epsilon: 0.12\n",
      "Episode 217000, Loss: 10.6206, Total Reward: 485.70, Epsilon: 0.11\n",
      "Episode 218000, Loss: 16.4226, Total Reward: 158.50, Epsilon: 0.11\n",
      "Episode 219000, Loss: 14.8076, Total Reward: 572.00, Epsilon: 0.11\n",
      "Model saved to snake_dqn_model_220000.pth\n",
      "Episode 220000, Loss: 11.2331, Total Reward: 161.60, Epsilon: 0.11\n",
      "Episode 221000, Loss: 9.1809, Total Reward: 111.20, Epsilon: 0.11\n",
      "Episode 222000, Loss: 17.5665, Total Reward: 19.30, Epsilon: 0.11\n",
      "Episode 223000, Loss: 10.2269, Total Reward: 415.30, Epsilon: 0.11\n",
      "Episode 224000, Loss: 11.6598, Total Reward: 532.70, Epsilon: 0.11\n",
      "Model saved to snake_dqn_model_225000.pth\n",
      "Episode 225000, Loss: 11.4883, Total Reward: 792.30, Epsilon: 0.11\n",
      "Episode 226000, Loss: 19.5732, Total Reward: 626.30, Epsilon: 0.10\n",
      "Episode 227000, Loss: 8.2150, Total Reward: 22.90, Epsilon: 0.10\n",
      "Episode 228000, Loss: 14.1085, Total Reward: 123.70, Epsilon: 0.10\n",
      "Episode 229000, Loss: 14.9956, Total Reward: 19.70, Epsilon: 0.10\n",
      "Model saved to snake_dqn_model_230000.pth\n",
      "Episode 230000, Loss: 19.0607, Total Reward: 1023.50, Epsilon: 0.10\n",
      "Episode 231000, Loss: 16.3780, Total Reward: 602.70, Epsilon: 0.10\n",
      "Episode 232000, Loss: 12.4396, Total Reward: 434.40, Epsilon: 0.10\n",
      "Episode 233000, Loss: 16.4718, Total Reward: 1789.00, Epsilon: 0.10\n",
      "Episode 234000, Loss: 10.8631, Total Reward: 348.30, Epsilon: 0.10\n",
      "Model saved to snake_dqn_model_235000.pth\n",
      "Episode 235000, Loss: 21.0529, Total Reward: 383.50, Epsilon: 0.10\n",
      "Episode 236000, Loss: 19.2048, Total Reward: 459.00, Epsilon: 0.09\n",
      "Episode 237000, Loss: 12.1462, Total Reward: 213.30, Epsilon: 0.09\n",
      "New best score: 30. Model saved to best_snake_dqn_model.pth\n",
      "Episode 238000, Loss: 22.5862, Total Reward: 309.10, Epsilon: 0.09\n",
      "Episode 239000, Loss: 12.3976, Total Reward: 689.30, Epsilon: 0.09\n",
      "Model saved to snake_dqn_model_240000.pth\n",
      "Episode 240000, Loss: 19.9287, Total Reward: 510.90, Epsilon: 0.09\n",
      "Episode 241000, Loss: 16.0188, Total Reward: 532.70, Epsilon: 0.09\n",
      "New best score: 31. Model saved to best_snake_dqn_model.pth\n",
      "Episode 242000, Loss: 19.8087, Total Reward: 112.40, Epsilon: 0.09\n",
      "Episode 243000, Loss: 31.7091, Total Reward: 1151.10, Epsilon: 0.09\n",
      "Episode 244000, Loss: 24.6621, Total Reward: 797.90, Epsilon: 0.09\n",
      "Model saved to snake_dqn_model_245000.pth\n",
      "Episode 245000, Loss: 10.9868, Total Reward: 403.50, Epsilon: 0.09\n",
      "Episode 246000, Loss: 12.9468, Total Reward: 610.60, Epsilon: 0.09\n",
      "Episode 247000, Loss: 20.7809, Total Reward: 406.30, Epsilon: 0.08\n",
      "Episode 248000, Loss: 10.1629, Total Reward: 21.40, Epsilon: 0.08\n",
      "Episode 249000, Loss: 16.7637, Total Reward: 86.10, Epsilon: 0.08\n",
      "Model saved to snake_dqn_model_250000.pth\n",
      "Episode 250000, Loss: 11.2448, Total Reward: 175.50, Epsilon: 0.08\n",
      "Episode 251000, Loss: 12.5907, Total Reward: 382.80, Epsilon: 0.08\n",
      "Episode 252000, Loss: 20.2366, Total Reward: 24.60, Epsilon: 0.08\n",
      "Episode 253000, Loss: 9.3415, Total Reward: 320.50, Epsilon: 0.08\n",
      "Episode 254000, Loss: 11.4152, Total Reward: 158.70, Epsilon: 0.08\n",
      "Model saved to snake_dqn_model_255000.pth\n",
      "Episode 255000, Loss: 8.1119, Total Reward: 525.40, Epsilon: 0.08\n",
      "New best score: 32. Model saved to best_snake_dqn_model.pth\n",
      "Episode 256000, Loss: 13.8057, Total Reward: 1547.20, Epsilon: 0.08\n",
      "Episode 257000, Loss: 8.6409, Total Reward: 675.70, Epsilon: 0.08\n",
      "Episode 258000, Loss: 9.5956, Total Reward: 537.30, Epsilon: 0.08\n",
      "Episode 259000, Loss: 24.2169, Total Reward: 469.90, Epsilon: 0.08\n",
      "Model saved to snake_dqn_model_260000.pth\n",
      "Episode 260000, Loss: 18.6292, Total Reward: 108.70, Epsilon: 0.07\n",
      "Episode 261000, Loss: 7.4608, Total Reward: 116.20, Epsilon: 0.07\n",
      "Episode 262000, Loss: 14.6427, Total Reward: 325.80, Epsilon: 0.07\n",
      "New best score: 35. Model saved to best_snake_dqn_model.pth\n",
      "Episode 263000, Loss: 14.6114, Total Reward: 257.80, Epsilon: 0.07\n",
      "Episode 264000, Loss: 29.8525, Total Reward: 652.50, Epsilon: 0.07\n",
      "Model saved to snake_dqn_model_265000.pth\n",
      "Episode 265000, Loss: 21.1656, Total Reward: 553.40, Epsilon: 0.07\n",
      "Episode 266000, Loss: 8.0012, Total Reward: 72.10, Epsilon: 0.07\n",
      "Episode 267000, Loss: 11.3618, Total Reward: 164.10, Epsilon: 0.07\n",
      "Episode 268000, Loss: 20.1152, Total Reward: 464.30, Epsilon: 0.07\n",
      "Episode 269000, Loss: 24.6938, Total Reward: 440.70, Epsilon: 0.07\n",
      "Model saved to snake_dqn_model_270000.pth\n",
      "Episode 270000, Loss: 8.7878, Total Reward: 664.40, Epsilon: 0.07\n",
      "Episode 271000, Loss: 15.5224, Total Reward: 121.80, Epsilon: 0.07\n",
      "Episode 272000, Loss: 13.4275, Total Reward: 1012.40, Epsilon: 0.07\n",
      "New best score: 36. Model saved to best_snake_dqn_model.pth\n",
      "Episode 273000, Loss: 10.1116, Total Reward: 129.80, Epsilon: 0.07\n",
      "Episode 274000, Loss: 19.1363, Total Reward: 1128.00, Epsilon: 0.06\n",
      "Model saved to snake_dqn_model_275000.pth\n",
      "Episode 275000, Loss: 32.0128, Total Reward: 469.50, Epsilon: 0.06\n",
      "Episode 276000, Loss: 26.0766, Total Reward: 879.10, Epsilon: 0.06\n",
      "Episode 277000, Loss: 20.2507, Total Reward: 1118.90, Epsilon: 0.06\n",
      "Episode 278000, Loss: 13.9737, Total Reward: 964.90, Epsilon: 0.06\n",
      "Episode 279000, Loss: 14.3705, Total Reward: 579.30, Epsilon: 0.06\n",
      "Model saved to snake_dqn_model_280000.pth\n",
      "Episode 280000, Loss: 67.2284, Total Reward: 262.70, Epsilon: 0.06\n",
      "Episode 281000, Loss: 23.9658, Total Reward: 993.60, Epsilon: 0.06\n",
      "Episode 282000, Loss: 27.3570, Total Reward: 715.50, Epsilon: 0.06\n",
      "Episode 283000, Loss: 13.5778, Total Reward: 394.60, Epsilon: 0.06\n",
      "Episode 284000, Loss: 8.8222, Total Reward: 251.30, Epsilon: 0.06\n",
      "Model saved to snake_dqn_model_285000.pth\n",
      "Episode 285000, Loss: 85.3108, Total Reward: 1094.20, Epsilon: 0.06\n",
      "Episode 286000, Loss: 10.9453, Total Reward: 257.80, Epsilon: 0.06\n",
      "Episode 287000, Loss: 17.3759, Total Reward: 754.00, Epsilon: 0.06\n",
      "Episode 288000, Loss: 17.2391, Total Reward: 943.40, Epsilon: 0.06\n",
      "Episode 289000, Loss: 13.8617, Total Reward: 171.60, Epsilon: 0.06\n",
      "Model saved to snake_dqn_model_290000.pth\n",
      "Episode 290000, Loss: 37.1409, Total Reward: 66.60, Epsilon: 0.06\n",
      "Episode 291000, Loss: 22.3437, Total Reward: 465.20, Epsilon: 0.05\n",
      "Episode 292000, Loss: 25.5282, Total Reward: 463.90, Epsilon: 0.05\n",
      "Episode 293000, Loss: 14.4876, Total Reward: 331.90, Epsilon: 0.05\n",
      "Episode 294000, Loss: 10.1930, Total Reward: 66.30, Epsilon: 0.05\n",
      "Model saved to snake_dqn_model_295000.pth\n",
      "Episode 295000, Loss: 27.3837, Total Reward: 848.70, Epsilon: 0.05\n",
      "Episode 296000, Loss: 19.4323, Total Reward: 69.50, Epsilon: 0.05\n",
      "Episode 297000, Loss: 17.6030, Total Reward: 470.40, Epsilon: 0.05\n",
      "Episode 298000, Loss: 31.1547, Total Reward: 469.50, Epsilon: 0.05\n",
      "Episode 299000, Loss: 38.5206, Total Reward: 412.60, Epsilon: 0.05\n",
      "Model saved to snake_dqn_model_300000.pth\n",
      "Episode 300000, Loss: 15.9176, Total Reward: 271.70, Epsilon: 0.05\n",
      "Episode 301000, Loss: 30.9519, Total Reward: 769.10, Epsilon: 0.05\n",
      "New best score: 37. Model saved to best_snake_dqn_model.pth\n",
      "Episode 302000, Loss: 42.3134, Total Reward: 117.50, Epsilon: 0.05\n",
      "Episode 303000, Loss: 12.1787, Total Reward: 67.60, Epsilon: 0.05\n",
      "Episode 304000, Loss: 12.7625, Total Reward: 113.00, Epsilon: 0.05\n",
      "Model saved to snake_dqn_model_305000.pth\n",
      "Episode 305000, Loss: 14.2732, Total Reward: 48.60, Epsilon: 0.05\n",
      "Episode 306000, Loss: 18.5275, Total Reward: 551.20, Epsilon: 0.05\n",
      "Episode 307000, Loss: 23.6366, Total Reward: 627.60, Epsilon: 0.05\n",
      "Episode 308000, Loss: 24.8500, Total Reward: 818.90, Epsilon: 0.05\n",
      "Episode 309000, Loss: 57.1341, Total Reward: 925.00, Epsilon: 0.05\n",
      "Model saved to snake_dqn_model_310000.pth\n",
      "Episode 310000, Loss: 27.6356, Total Reward: 958.20, Epsilon: 0.05\n",
      "Episode 311000, Loss: 10.8916, Total Reward: 443.20, Epsilon: 0.04\n",
      "Episode 312000, Loss: 17.3094, Total Reward: 836.70, Epsilon: 0.04\n",
      "Episode 313000, Loss: 34.3714, Total Reward: 941.90, Epsilon: 0.04\n",
      "Episode 314000, Loss: 8.0246, Total Reward: 354.40, Epsilon: 0.04\n",
      "Model saved to snake_dqn_model_315000.pth\n",
      "Episode 315000, Loss: 7.0491, Total Reward: 160.20, Epsilon: 0.04\n",
      "Episode 316000, Loss: 13.2788, Total Reward: 1027.50, Epsilon: 0.04\n",
      "Episode 317000, Loss: 16.8094, Total Reward: 922.90, Epsilon: 0.04\n",
      "Episode 318000, Loss: 56.9918, Total Reward: 278.50, Epsilon: 0.04\n",
      "Episode 319000, Loss: 14.9877, Total Reward: 611.00, Epsilon: 0.04\n",
      "Model saved to snake_dqn_model_320000.pth\n",
      "Episode 320000, Loss: 15.1734, Total Reward: 136.90, Epsilon: 0.04\n",
      "New best score: 38. Model saved to best_snake_dqn_model.pth\n",
      "Episode 321000, Loss: 15.1834, Total Reward: 904.50, Epsilon: 0.04\n",
      "Episode 322000, Loss: 40.2448, Total Reward: 24.60, Epsilon: 0.04\n",
      "New best score: 39. Model saved to best_snake_dqn_model.pth\n",
      "Episode 323000, Loss: 13.6642, Total Reward: 193.80, Epsilon: 0.04\n",
      "Episode 324000, Loss: 14.8747, Total Reward: 1121.90, Epsilon: 0.04\n",
      "Model saved to snake_dqn_model_325000.pth\n",
      "Episode 325000, Loss: 18.5070, Total Reward: 902.30, Epsilon: 0.04\n",
      "Episode 326000, Loss: 16.1945, Total Reward: 582.80, Epsilon: 0.04\n",
      "Episode 327000, Loss: 14.3427, Total Reward: 427.10, Epsilon: 0.04\n",
      "Episode 328000, Loss: 15.3818, Total Reward: 762.60, Epsilon: 0.04\n",
      "Episode 329000, Loss: 21.3673, Total Reward: 1342.50, Epsilon: 0.04\n",
      "Model saved to snake_dqn_model_330000.pth\n",
      "Episode 330000, Loss: 10.1890, Total Reward: 1210.20, Epsilon: 0.04\n",
      "Episode 331000, Loss: 16.6770, Total Reward: 684.80, Epsilon: 0.04\n",
      "Episode 332000, Loss: 12.6037, Total Reward: 765.30, Epsilon: 0.04\n",
      "Episode 333000, Loss: 29.0946, Total Reward: 551.00, Epsilon: 0.04\n",
      "Episode 334000, Loss: 37.4121, Total Reward: 1379.30, Epsilon: 0.04\n",
      "Model saved to snake_dqn_model_335000.pth\n",
      "Episode 335000, Loss: 12.8653, Total Reward: 60.60, Epsilon: 0.04\n",
      "Episode 336000, Loss: 38.0347, Total Reward: 162.40, Epsilon: 0.03\n",
      "New best score: 40. Model saved to best_snake_dqn_model.pth\n",
      "Episode 337000, Loss: 13.7763, Total Reward: 8942.10, Epsilon: 0.03\n",
      "Episode 338000, Loss: 23.2596, Total Reward: 69.50, Epsilon: 0.03\n",
      "Episode 339000, Loss: 12.3926, Total Reward: 678.40, Epsilon: 0.03\n",
      "Model saved to snake_dqn_model_340000.pth\n",
      "Episode 340000, Loss: 30.0265, Total Reward: 385.50, Epsilon: 0.03\n",
      "New best score: 47. Model saved to best_snake_dqn_model.pth\n",
      "Episode 341000, Loss: 146.9285, Total Reward: 264.10, Epsilon: 0.03\n",
      "Episode 342000, Loss: 21.3668, Total Reward: 1070.20, Epsilon: 0.03\n",
      "Episode 343000, Loss: 51.1909, Total Reward: 23.20, Epsilon: 0.03\n",
      "Episode 344000, Loss: 31.0772, Total Reward: 821.80, Epsilon: 0.03\n",
      "Model saved to snake_dqn_model_345000.pth\n",
      "Episode 345000, Loss: 19.3006, Total Reward: 75.00, Epsilon: 0.03\n",
      "Episode 346000, Loss: 15.5911, Total Reward: 321.80, Epsilon: 0.03\n",
      "Episode 347000, Loss: 25.8127, Total Reward: 498.40, Epsilon: 0.03\n",
      "Episode 348000, Loss: 45.2444, Total Reward: 504.80, Epsilon: 0.03\n",
      "Episode 349000, Loss: 16.0846, Total Reward: 671.40, Epsilon: 0.03\n",
      "Model saved to snake_dqn_model_350000.pth\n",
      "Episode 350000, Loss: 46.3778, Total Reward: 1302.10, Epsilon: 0.03\n",
      "Episode 351000, Loss: 37.5043, Total Reward: 1603.00, Epsilon: 0.03\n",
      "Episode 352000, Loss: 10.2433, Total Reward: 1158.80, Epsilon: 0.03\n",
      "Episode 353000, Loss: 35.1051, Total Reward: 706.70, Epsilon: 0.03\n",
      "Episode 354000, Loss: 13.7047, Total Reward: 1081.30, Epsilon: 0.03\n",
      "Model saved to snake_dqn_model_355000.pth\n",
      "Episode 355000, Loss: 28.1364, Total Reward: 70.70, Epsilon: 0.03\n",
      "Episode 356000, Loss: 29.2247, Total Reward: 758.80, Epsilon: 0.03\n",
      "Episode 357000, Loss: 50.4168, Total Reward: 532.90, Epsilon: 0.03\n",
      "Episode 358000, Loss: 11.9382, Total Reward: 735.20, Epsilon: 0.03\n",
      "Episode 359000, Loss: 28.8866, Total Reward: 1168.00, Epsilon: 0.03\n",
      "Model saved to snake_dqn_model_360000.pth\n",
      "Episode 360000, Loss: 19.8698, Total Reward: 323.30, Epsilon: 0.03\n",
      "Episode 361000, Loss: 25.3184, Total Reward: 72.10, Epsilon: 0.03\n",
      "Episode 362000, Loss: 32.0999, Total Reward: 452.10, Epsilon: 0.03\n",
      "Episode 363000, Loss: 78.4789, Total Reward: 1310.60, Epsilon: 0.03\n",
      "Episode 364000, Loss: 27.7658, Total Reward: 1363.40, Epsilon: 0.03\n",
      "Model saved to snake_dqn_model_365000.pth\n",
      "Episode 365000, Loss: 65.0886, Total Reward: 504.80, Epsilon: 0.03\n",
      "Episode 366000, Loss: 24.1898, Total Reward: 1256.00, Epsilon: 0.03\n",
      "Episode 367000, Loss: 41.0816, Total Reward: 1439.60, Epsilon: 0.03\n",
      "Episode 368000, Loss: 38.3503, Total Reward: 675.90, Epsilon: 0.03\n",
      "Episode 369000, Loss: 24.9277, Total Reward: 667.80, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_370000.pth\n",
      "Episode 370000, Loss: 29.1279, Total Reward: 1084.10, Epsilon: 0.02\n",
      "Episode 371000, Loss: 86.2956, Total Reward: 1165.60, Epsilon: 0.02\n",
      "Episode 372000, Loss: 67.2089, Total Reward: 612.10, Epsilon: 0.02\n",
      "Episode 373000, Loss: 23.5016, Total Reward: 789.20, Epsilon: 0.02\n",
      "Episode 374000, Loss: 18.7631, Total Reward: 1215.00, Epsilon: 0.02\n",
      "Episode 374290 skipped due to being stuck.\n",
      "Model saved to snake_dqn_model_375000.pth\n",
      "Episode 375000, Loss: 22.4951, Total Reward: 1034.60, Epsilon: 0.02\n",
      "Episode 376000, Loss: 37.1539, Total Reward: 1109.40, Epsilon: 0.02\n",
      "Episode 377000, Loss: 15.9197, Total Reward: 566.10, Epsilon: 0.02\n",
      "Episode 378000, Loss: 22.8849, Total Reward: 264.90, Epsilon: 0.02\n",
      "Episode 379000, Loss: 32.0737, Total Reward: 294.80, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_380000.pth\n",
      "Episode 380000, Loss: 52.9403, Total Reward: 1829.70, Epsilon: 0.02\n",
      "Episode 381000, Loss: 25.3375, Total Reward: 626.50, Epsilon: 0.02\n",
      "Episode 382000, Loss: 11.9426, Total Reward: 586.40, Epsilon: 0.02\n",
      "Episode 383000, Loss: 43.8192, Total Reward: 330.00, Epsilon: 0.02\n",
      "Episode 384000, Loss: 321.8274, Total Reward: 177.70, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_385000.pth\n",
      "Episode 385000, Loss: 34.7458, Total Reward: 1104.50, Epsilon: 0.02\n",
      "Episode 386000, Loss: 86.1915, Total Reward: 396.90, Epsilon: 0.02\n",
      "Episode 387000, Loss: 197.9607, Total Reward: 1023.80, Epsilon: 0.02\n",
      "Episode 388000, Loss: 16.0797, Total Reward: 680.40, Epsilon: 0.02\n",
      "Episode 389000, Loss: 75.2090, Total Reward: 1057.20, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_390000.pth\n",
      "Episode 390000, Loss: 31.3256, Total Reward: 511.00, Epsilon: 0.02\n",
      "Training has stagnated for 50000 episodes. Learning rate is halved.\n",
      "Episode 391000, Loss: 112.5162, Total Reward: 9198.00, Epsilon: 0.02\n",
      "Episode 392000, Loss: 149.0292, Total Reward: 1072.60, Epsilon: 0.02\n",
      "Episode 393000, Loss: 30.8853, Total Reward: 948.80, Epsilon: 0.02\n",
      "Episode 394000, Loss: 29.7325, Total Reward: 954.30, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_395000.pth\n",
      "Episode 395000, Loss: 94.6561, Total Reward: 578.00, Epsilon: 0.02\n",
      "Episode 396000, Loss: 156.9307, Total Reward: 902.80, Epsilon: 0.02\n",
      "Episode 397000, Loss: 43.8882, Total Reward: 269.30, Epsilon: 0.02\n",
      "Episode 398000, Loss: 52.5870, Total Reward: 701.20, Epsilon: 0.02\n",
      "Episode 399000, Loss: 49.6427, Total Reward: 205.00, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_400000.pth\n",
      "Episode 400000, Loss: 21.5173, Total Reward: 1420.00, Epsilon: 0.02\n",
      "Episode 401000, Loss: 39.3567, Total Reward: 1744.20, Epsilon: 0.02\n",
      "Episode 402000, Loss: 120.9395, Total Reward: 583.40, Epsilon: 0.02\n",
      "Episode 403000, Loss: 727.8168, Total Reward: 1281.30, Epsilon: 0.02\n",
      "Episode 404000, Loss: 35.3471, Total Reward: 481.00, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_405000.pth\n",
      "Episode 405000, Loss: 163.4362, Total Reward: 1515.80, Epsilon: 0.02\n",
      "Episode 406000, Loss: 192.0534, Total Reward: 463.60, Epsilon: 0.02\n",
      "Episode 407000, Loss: 39.1564, Total Reward: 217.00, Epsilon: 0.02\n",
      "Episode 408000, Loss: 44.2662, Total Reward: 1495.10, Epsilon: 0.02\n",
      "Episode 409000, Loss: 486.4222, Total Reward: 718.40, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_410000.pth\n",
      "Episode 410000, Loss: 40.2413, Total Reward: 515.10, Epsilon: 0.02\n",
      "Episode 411000, Loss: 115.0279, Total Reward: 409.60, Epsilon: 0.02\n",
      "Episode 412000, Loss: 110.0067, Total Reward: 962.30, Epsilon: 0.02\n",
      "Episode 413000, Loss: 81.1109, Total Reward: 304.40, Epsilon: 0.02\n",
      "Episode 414000, Loss: 36.6769, Total Reward: 586.30, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_415000.pth\n",
      "Episode 415000, Loss: 34.5528, Total Reward: 370.00, Epsilon: 0.02\n",
      "Episode 416000, Loss: 63.2004, Total Reward: 446.40, Epsilon: 0.02\n",
      "Episode 417000, Loss: 84.6396, Total Reward: 843.50, Epsilon: 0.02\n",
      "Episode 418000, Loss: 34.0810, Total Reward: 351.10, Epsilon: 0.02\n",
      "Episode 419000, Loss: 93.0494, Total Reward: 944.80, Epsilon: 0.02\n",
      "Model saved to snake_dqn_model_420000.pth\n",
      "Episode 420000, Loss: 110.5887, Total Reward: 611.60, Epsilon: 0.01\n",
      "Episode 421000, Loss: 32.8431, Total Reward: 2064.30, Epsilon: 0.01\n",
      "Episode 422000, Loss: 150.5081, Total Reward: 341.10, Epsilon: 0.01\n",
      "Episode 423000, Loss: 82.5265, Total Reward: 437.10, Epsilon: 0.01\n",
      "Episode 424000, Loss: 54.0583, Total Reward: 293.50, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_425000.pth\n",
      "Episode 425000, Loss: 118.0975, Total Reward: 1284.30, Epsilon: 0.01\n",
      "Episode 426000, Loss: 77.2681, Total Reward: 586.40, Epsilon: 0.01\n",
      "Episode 427000, Loss: 71.5526, Total Reward: 1173.60, Epsilon: 0.01\n",
      "Episode 428000, Loss: 210.9336, Total Reward: 1089.90, Epsilon: 0.01\n",
      "Episode 429000, Loss: 62.7485, Total Reward: 745.40, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_430000.pth\n",
      "Episode 430000, Loss: 30.5282, Total Reward: 1181.20, Epsilon: 0.01\n",
      "Episode 431000, Loss: 28.9678, Total Reward: 1576.70, Epsilon: 0.01\n",
      "Episode 432000, Loss: 30.0462, Total Reward: 558.40, Epsilon: 0.01\n",
      "Episode 433000, Loss: 30.4752, Total Reward: 1176.40, Epsilon: 0.01\n",
      "Episode 434000, Loss: 82.3419, Total Reward: 340.00, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_435000.pth\n",
      "Episode 435000, Loss: 34.2924, Total Reward: 1111.70, Epsilon: 0.01\n",
      "Episode 436000, Loss: 43.5224, Total Reward: 1154.90, Epsilon: 0.01\n",
      "Episode 437000, Loss: 47.5576, Total Reward: 19.10, Epsilon: 0.01\n",
      "Episode 438000, Loss: 173.7119, Total Reward: 873.60, Epsilon: 0.01\n",
      "Episode 439000, Loss: 50.1557, Total Reward: 764.20, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_440000.pth\n",
      "Episode 440000, Loss: 25.0053, Total Reward: 814.40, Epsilon: 0.01\n",
      "Training has stagnated for 50000 episodes. Learning rate is halved.\n",
      "Episode 441000, Loss: 84.2918, Total Reward: 584.70, Epsilon: 0.01\n",
      "Episode 442000, Loss: 142.9584, Total Reward: 850.30, Epsilon: 0.01\n",
      "Episode 443000, Loss: 45.5429, Total Reward: 760.20, Epsilon: 0.01\n",
      "Episode 444000, Loss: 88.3800, Total Reward: 423.20, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_445000.pth\n",
      "Episode 445000, Loss: 18.5130, Total Reward: 665.50, Epsilon: 0.01\n",
      "Episode 446000, Loss: 81.0921, Total Reward: 1445.70, Epsilon: 0.01\n",
      "Episode 447000, Loss: 177.6361, Total Reward: 983.40, Epsilon: 0.01\n",
      "Episode 448000, Loss: 68.4193, Total Reward: 445.70, Epsilon: 0.01\n",
      "Episode 449000, Loss: 69.6565, Total Reward: 1442.10, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_450000.pth\n",
      "Episode 450000, Loss: 76.4591, Total Reward: 672.20, Epsilon: 0.01\n",
      "Episode 451000, Loss: 44.7976, Total Reward: 31934.50, Epsilon: 0.01\n",
      "Episode 452000, Loss: 41.9615, Total Reward: 608.80, Epsilon: 0.01\n",
      "Episode 453000, Loss: 91.0773, Total Reward: 952.40, Epsilon: 0.01\n",
      "Episode 454000, Loss: 296.3308, Total Reward: 1587.70, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_455000.pth\n",
      "Episode 455000, Loss: 178.0222, Total Reward: 621.60, Epsilon: 0.01\n",
      "Episode 456000, Loss: 117.4465, Total Reward: 1283.80, Epsilon: 0.01\n",
      "Episode 456290 skipped due to being stuck.\n",
      "Episode 457000, Loss: 28.1113, Total Reward: 759.10, Epsilon: 0.01\n",
      "Episode 458000, Loss: 51.4631, Total Reward: 565.00, Epsilon: 0.01\n",
      "Episode 459000, Loss: 177.4871, Total Reward: 1134.20, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_460000.pth\n",
      "Episode 460000, Loss: 308.7495, Total Reward: 661.30, Epsilon: 0.01\n",
      "Episode 461000, Loss: 51.1033, Total Reward: 1274.50, Epsilon: 0.01\n",
      "Episode 462000, Loss: 81.5145, Total Reward: 66.90, Epsilon: 0.01\n",
      "Episode 463000, Loss: 428.3539, Total Reward: 166.20, Epsilon: 0.01\n",
      "Episode 464000, Loss: 230.1291, Total Reward: 1752.40, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_465000.pth\n",
      "Episode 465000, Loss: 158.8168, Total Reward: 932.00, Epsilon: 0.01\n",
      "Episode 466000, Loss: 279.7736, Total Reward: 337.80, Epsilon: 0.01\n",
      "Episode 466920 skipped due to being stuck.\n",
      "Episode 467000, Loss: 112.7378, Total Reward: 653.10, Epsilon: 0.01\n",
      "Episode 468000, Loss: 350.6992, Total Reward: 1310.00, Epsilon: 0.01\n",
      "Episode 469000, Loss: 47.7887, Total Reward: 1101.10, Epsilon: 0.01\n",
      "Model saved to snake_dqn_model_470000.pth\n",
      "Episode 470000, Loss: 1440.6697, Total Reward: 849.60, Epsilon: 0.01\n",
      "Episode 471000, Loss: 93.2498, Total Reward: 1957.40, Epsilon: 0.01\n",
      "Episode 472000, Loss: 421.3374, Total Reward: 1529.00, Epsilon: 0.01\n",
      "Episode 473000, Loss: 447.8214, Total Reward: 1513.40, Epsilon: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 408\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete. Best score (number of food eaten): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[43mtrain_snake_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 360\u001b[0m, in \u001b[0;36mtrain_snake_game\u001b[1;34m(render)\u001b[0m\n\u001b[0;32m    358\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    359\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_experience((state, action, reward, next_state, done))\n\u001b[1;32m--> 360\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    362\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[11], line 311\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m batch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer, BATCH_SIZE)\n\u001b[0;32m    309\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m--> 311\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    312\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(actions), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    313\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(rewards), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import pygame\n",
    "import torch.nn.functional as F  # Import functional module\n",
    "import cv2\n",
    "\n",
    "# Define constants\n",
    "GRID_SIZE = 20\n",
    "PIXEL_SIZE = 30\n",
    "FOOD_REWARD = 50\n",
    "MOVE_PENALTY = -0.1\n",
    "CLOSER_REWARD = 1.2\n",
    "FURTHER_PENALTY = -1\n",
    "DEATH_PENALTY = -20\n",
    "TIME_PENALTY = -0.1\n",
    "MIN_EPSILON = 0.0\n",
    "FPS = 60\n",
    "EXPERIENCE_REPLAY_SIZE = 1000000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.8\n",
    "MODEL_SAVE_INTERVAL = 5000\n",
    "LEARNING_RATE = 0.00005\n",
    "EPISODES = 1000000  # Set the number of episodes for training\n",
    "STUCK_THRESHOLD = 2500  # Number of steps to consider the snake as stuck\n",
    "DECAY_RATE = 0.99999  # Epsilon decay rate\n",
    "STAGNATION_THRESHOLD = 50000  # Number of episodes for stagnation\n",
    "\n",
    "# Colors\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "\n",
    "class SnakeGame:\n",
    "    def __init__(self, render=True):\n",
    "        self.grid_size = GRID_SIZE\n",
    "        self.pixel_size = PIXEL_SIZE\n",
    "        self.render_enabled = render\n",
    "        if self.render_enabled:\n",
    "            self.screen = pygame.display.set_mode((self.grid_size * self.pixel_size, self.grid_size * self.pixel_size))\n",
    "            pygame.display.set_caption('Improved Snake Game with DQN')\n",
    "        self.reset()\n",
    "        self.clock = pygame.time.Clock()  # For controlling the FPS\n",
    "\n",
    "    def reset(self):\n",
    "        self.snake = [(self.grid_size // 2, self.grid_size // 2)]\n",
    "        self.direction = (0, 1)  # Initial direction is moving right\n",
    "        self.place_food()\n",
    "        self.score = 0\n",
    "        self.steps_since_food = 0\n",
    "        self.previous_distance = self.get_distance(self.snake[0], self.food)\n",
    "        self.steps_since_last_progress = 0  # Initialize steps since last progress\n",
    "        return self.get_state()\n",
    "\n",
    "    def place_food(self):\n",
    "        while True:\n",
    "            self.food = (random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1))\n",
    "            if self.food not in self.snake:\n",
    "                break\n",
    "    \n",
    "    def get_state(self):\n",
    "        head_x, head_y = self.snake[0]\n",
    "        food_x, food_y = self.food\n",
    "\n",
    "        # Calculate normalized positions\n",
    "        head_x_norm = head_x / self.grid_size\n",
    "        head_y_norm = head_y / self.grid_size\n",
    "        food_x_norm = food_x / self.grid_size\n",
    "        food_y_norm = food_y / self.grid_size\n",
    "\n",
    "        # Direction vectors (normalized)\n",
    "        direction_x, direction_y = self.direction\n",
    "        magnitude_direction = np.sqrt(direction_x**2 + direction_y**2)\n",
    "        if magnitude_direction != 0:\n",
    "            direction_x /= magnitude_direction\n",
    "            direction_y /= magnitude_direction\n",
    "\n",
    "        # Distance to food (normalized)\n",
    "        distance_to_food = self.get_distance(self.snake[0], self.food) / (self.grid_size * 2)\n",
    "\n",
    "        # Angle to food (normalized)\n",
    "        relative_food_x = food_x - head_x\n",
    "        relative_food_y = food_y - head_y\n",
    "        dot_product = direction_x * relative_food_x + direction_y * relative_food_y\n",
    "        magnitude_food = np.sqrt(relative_food_x**2 + relative_food_y**2)\n",
    "        angle_to_food = np.arccos(np.clip(dot_product / magnitude_food, -1.0, 1.0)) / np.pi if magnitude_food != 0 else 0\n",
    "\n",
    "        # Normalize distances to the nearest walls\n",
    "        distance_to_left_wall = head_x / self.grid_size\n",
    "        distance_to_right_wall = (self.grid_size - 1 - head_x) / self.grid_size\n",
    "        distance_to_top_wall = head_y / self.grid_size\n",
    "        distance_to_bottom_wall = (self.grid_size - 1 - head_y) / self.grid_size\n",
    "\n",
    "        # Distance to the closest body segment\n",
    "        distances_to_body_segments = [self.get_distance(self.snake[0], segment) for segment in self.snake[1:]]\n",
    "        distance_to_closest_body_segment = min(distances_to_body_segments) / self.grid_size if distances_to_body_segments else 1\n",
    "\n",
    "        # Snake length (normalized)\n",
    "        snake_length = len(self.snake) / (self.grid_size * 2)\n",
    "\n",
    "        # Danger detection: Straight, Left, Right\n",
    "        danger_straight = self.is_collision((head_x + self.direction[0], head_y + self.direction[1]))\n",
    "\n",
    "        left_direction = (-self.direction[1], self.direction[0])  # Left turn\n",
    "        danger_left = self.is_collision((head_x + left_direction[0], head_y + left_direction[1]))\n",
    "\n",
    "        right_direction = (self.direction[1], -self.direction[0])  # Right turn\n",
    "        danger_right = self.is_collision((head_x + right_direction[0], head_y + right_direction[1]))\n",
    "\n",
    "        # Current direction (encoded as one-hot)\n",
    "        direction_left = int(self.direction == (-1, 0))\n",
    "        direction_right = int(self.direction == (1, 0))\n",
    "        direction_up = int(self.direction == (0, -1))\n",
    "        direction_down = int(self.direction == (0, 1))\n",
    "\n",
    "        # Calculate the relative direction of food\n",
    "        food_left = int(food_x < head_x)\n",
    "        food_right = int(food_x > head_x)\n",
    "        food_up = int(food_y < head_y)\n",
    "        food_down = int(food_y > head_y)\n",
    "\n",
    "        # Final state vector: Combined features\n",
    "        state = np.array([\n",
    "            head_x_norm, head_y_norm,                 # Snake head position (normalized)\n",
    "            food_x_norm, food_y_norm,                 # Food position (normalized)\n",
    "            direction_x, direction_y,                 # Snake direction\n",
    "            distance_to_food,                         # Distance to food\n",
    "            angle_to_food,                            # Angle to food\n",
    "            distance_to_left_wall, distance_to_right_wall, distance_to_top_wall, distance_to_bottom_wall,  # Wall distances (normalized)\n",
    "            distance_to_closest_body_segment,         # Closest body segment\n",
    "            snake_length,                             # Snake length (normalized)\n",
    "            danger_straight, danger_left, danger_right,  # Dangers\n",
    "            direction_left, direction_right, direction_up, direction_down,  # Current direction (one-hot encoded)\n",
    "            food_left, food_right, food_up, food_down  # Food relative direction\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.change_direction(action)\n",
    "        new_head = (self.snake[0][0] + self.direction[0], self.snake[0][1] + self.direction[1])\n",
    "    \n",
    "        reward = MOVE_PENALTY\n",
    "        done = False\n",
    "    \n",
    "        if self.is_collision(new_head):\n",
    "            reward = DEATH_PENALTY\n",
    "            done = True\n",
    "        else:\n",
    "            self.snake = [new_head] + self.snake[:-1]\n",
    "            new_distance = self.get_distance(self.snake[0], self.food)\n",
    "            self.previous_distance = new_distance\n",
    "            self.steps_since_food += 1\n",
    "    \n",
    "            if new_head == self.food:\n",
    "                self.snake = [new_head] + self.snake\n",
    "                self.score += 1  # Update the score\n",
    "                reward = FOOD_REWARD\n",
    "                self.place_food()\n",
    "                self.steps_since_food = 0\n",
    "                self.previous_distance = self.get_distance(self.snake[0], self.food)\n",
    "            else:\n",
    "                old_distance = self.previous_distance\n",
    "                if new_distance < old_distance:\n",
    "                    reward += CLOSER_REWARD\n",
    "                else:\n",
    "                    reward += FURTHER_PENALTY\n",
    "    \n",
    "                reward += - (self.steps_since_food * TIME_PENALTY)\n",
    "    \n",
    "                if self.steps_since_last_progress >= STUCK_THRESHOLD:\n",
    "                    if self.detect_loop() or self.is_in_small_area():\n",
    "                        done = True\n",
    "                else:\n",
    "                    if new_distance != old_distance:\n",
    "                        self.steps_since_last_progress = 0\n",
    "                    else:\n",
    "                        self.steps_since_last_progress += 1\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "    \n",
    "    def detect_loop(self):\n",
    "        recent_positions = deque(maxlen=20)  # Track last 20 positions\n",
    "        recent_positions.append(self.snake[0])\n",
    "        \n",
    "        # Check if snake is revisiting recent positions too frequently\n",
    "        revisit_count = sum(1 for pos in self.snake[1:] if pos in recent_positions)\n",
    "        if revisit_count >= len(self.snake) / 2:  # Arbitrary threshold; adjust as needed\n",
    "            return True\n",
    "        \n",
    "        # Update previous distance\n",
    "        current_distance = self.get_distance(self.snake[0], self.food)\n",
    "        if abs(current_distance - self.previous_distance) < 1:\n",
    "            return True\n",
    "    \n",
    "        # Update previous distance for next check\n",
    "        self.previous_distance = current_distance\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def is_in_small_area(self):\n",
    "        # Check if the snake is confined to a small area\n",
    "        snake_positions = set(self.snake)\n",
    "        min_x = min(x for x, y in snake_positions)\n",
    "        max_x = max(x for x, y in snake_positions)\n",
    "        min_y = min(y for x, y in snake_positions)\n",
    "        max_y = max(y for x, y in snake_positions)\n",
    "        return (max_x - min_x <= 10) and (max_y - min_y <= 10)  # Adjust the area size threshold\n",
    "\n",
    "    def change_direction(self, action):\n",
    "        left_turns = {\n",
    "            (1, 0): (0, 1),   # Moving right, turn left -> down\n",
    "            (0, 1): (-1, 0),  # Moving down, turn left -> left\n",
    "            (-1, 0): (0, -1), # Moving left, turn left -> up\n",
    "            (0, -1): (1, 0)   # Moving up, turn left -> right\n",
    "        }\n",
    "        \n",
    "        right_turns = {\n",
    "            (1, 0): (0, -1),  # Moving right, turn right -> up\n",
    "            (0, 1): (1, 0),   # Moving down, turn right -> right\n",
    "            (-1, 0): (0, 1),  # Moving left, turn right -> down\n",
    "            (0, -1): (-1, 0)  # Moving up, turn right -> left\n",
    "        }\n",
    "        \n",
    "        if action == 0:  # Left\n",
    "            self.direction = left_turns[self.direction]\n",
    "        elif action == 1:  # Right\n",
    "            self.direction = right_turns[self.direction]\n",
    "        # No action means move straight (no change in direction)\n",
    "\n",
    "    def is_collision(self, position):\n",
    "        x, y = position\n",
    "        return (x < 0 or x >= self.grid_size or y < 0 or y >= self.grid_size or\n",
    "                position in self.snake)\n",
    "    \n",
    "    def get_distance(self, pos1, pos2):\n",
    "        x1, y1 = pos1\n",
    "        x2, y2 = pos2\n",
    "        return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "    \n",
    "    def render(self):\n",
    "        self.screen.fill(BLACK)\n",
    "        for segment in self.snake:\n",
    "            pygame.draw.rect(self.screen, GREEN, pygame.Rect(segment[0] * self.pixel_size, segment[1] * self.pixel_size, self.pixel_size, self.pixel_size))\n",
    "        pygame.draw.rect(self.screen, RED, pygame.Rect(self.food[0] * self.pixel_size, self.food[1] * self.pixel_size, self.pixel_size, self.pixel_size))\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(FPS)  # Cap the frame rate\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)  # Initial number of units\n",
    "        self.ln1 = nn.LayerNorm(1024)  # Use LayerNorm\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Dropout rate\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 1024)  # Fixed input size to match output of fc1\n",
    "        self.ln2 = nn.LayerNorm(1024)  # Use LayerNorm\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Dropout rate\n",
    "\n",
    "        self.fc3 = nn.Linear(1024, 1024)  # Fixed input size to match output of fc2\n",
    "        self.ln3 = nn.LayerNorm(1024)  # Use LayerNorm\n",
    "        self.dropout3 = nn.Dropout(0.1)  # Dropout rate\n",
    "\n",
    "        self.fc4 = nn.Linear(1024, output_dim)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.ln1(self.fc1(x)), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.ln2(self.fc2(x)), negative_slope=0.01)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.ln3(self.fc3(x)), negative_slope=0.01)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, action_dim, device):\n",
    "        self.device = device\n",
    "        self.action_dim = action_dim\n",
    "        self.q_network = DQN(input_dim, action_dim).to(device)\n",
    "        self.target_network = DQN(input_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=LEARNING_RATE)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10000, gamma=0.9)\n",
    "        self.replay_buffer = deque(maxlen=EXPERIENCE_REPLAY_SIZE)\n",
    "        self.update_target_network()\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)  # Random action\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return q_values.argmax().item()  # Best action\n",
    "\n",
    "    def store_experience(self, experience):\n",
    "        self.replay_buffer.append(experience)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return None  # Not enough data to train\n",
    "    \n",
    "        batch = random.sample(self.replay_buffer, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.int64).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(np.array(dones), dtype=torch.float32).to(self.device)\n",
    "    \n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "        # Double DQN\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.q_network(next_states).argmax(1)\n",
    "            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "    \n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)  # Gradient clipping\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()  # Step the learning rate scheduler\n",
    "    \n",
    "        return loss.item()  # Return the loss value\n",
    "\n",
    "def decay_epsilon(epsilon, episode, decay_rate=DECAY_RATE, min_epsilon=MIN_EPSILON):\n",
    "    return max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "def train_snake_game(render=False):\n",
    "    pygame.init()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_dim = 25\n",
    "    action_dim = 3\n",
    "    agent = DQNAgent(input_dim, action_dim, device)\n",
    "    game = SnakeGame(render=render)\n",
    "    epsilon = 1.0\n",
    "    best_score = 0\n",
    "    best_model_path = \"best_snake_dqn_model.pth\"\n",
    "    stagnation_counter = 0\n",
    "\n",
    "    for episode in range(EPISODES):\n",
    "        state = game.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        stuck_steps = 0\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            while not done and stuck_steps < STUCK_THRESHOLD:\n",
    "                action = agent.select_action(state, epsilon)\n",
    "                next_state, reward, done = game.step(action)\n",
    "                agent.store_experience((state, action, reward, next_state, done))\n",
    "                loss = agent.train()\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if render:\n",
    "                    game.render()\n",
    "\n",
    "                stuck_steps += 1\n",
    "\n",
    "            if stuck_steps >= STUCK_THRESHOLD:\n",
    "                print(f\"Episode {episode} skipped due to being stuck.\")\n",
    "                continue\n",
    "\n",
    "        if game.score > best_score:\n",
    "            best_score = game.score\n",
    "            torch.save(agent.q_network.state_dict(), best_model_path)\n",
    "            print(f\"New best score: {best_score}. Model saved to {best_model_path}\")\n",
    "            stagnation_counter = 0\n",
    "        else:\n",
    "            stagnation_counter += 1\n",
    "\n",
    "        if episode % MODEL_SAVE_INTERVAL == 0 and episode != 0:\n",
    "            model_path = f\"snake_dqn_model_{episode}.pth\"\n",
    "            torch.save(agent.q_network.state_dict(), model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            if loss is not None:\n",
    "                print(f\"Episode {episode}, Loss: {loss:.4f}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "            else:\n",
    "                print(f\"Episode {episode}, Loss: Not computed due to insufficient experience\")\n",
    "\n",
    "        if stagnation_counter >= STAGNATION_THRESHOLD:\n",
    "            print(f\"Training has stagnated for {STAGNATION_THRESHOLD} episodes. Learning rate is halved.\")\n",
    "            # LEARNING_RATE = LEARNING_RATE/2.0\n",
    "            # for param_group in agent.optimizer.param_groups:\n",
    "            #     param_group['lr'] = LEARNING_RATE\n",
    "            stagnation_counter = 0\n",
    "\n",
    "        epsilon = decay_epsilon(epsilon, episode)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "    pygame.quit()\n",
    "    print(f\"Training complete. Best score (number of food eaten): {best_score}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_snake_game(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540fcd3d-7ae1-4d70-bffd-737a9eaaa3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear the CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7515ac7-75aa-432a-9f56-854888bb7b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa660b4-a712-45ce-9cc0-8cbca2486086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5a4ca-020d-40a6-9ae4-5cfbbccb884e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7f9a1-6185-477d-bba1-e85f6055474b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27fb5d-9efb-417c-a5f9-f5a45d2700cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc8a9c-90ee-4cb2-a7bc-a56bdd8f50c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f2e13-6566-4dcd-9493-da493d99d84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
